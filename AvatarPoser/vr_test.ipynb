{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rnwns\\anaconda3\\envs\\pytorch_geometric\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "from human_body_prior.tools.rotation_tools import aa2matrot,matrot2aa,local2global_pose\n",
    "import random\n",
    "from utils import utils_transform\n",
    "from models.network import AvatarPoser as net\n",
    "from scipy import signal\n",
    "\n",
    "import glob\n",
    "from IPython import embed\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from human_body_prior.body_model.body_model import BodyModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvatarPoser(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layer, embed_dim, nhead, body_model, device):\n",
    "        super(AvatarPoser, self).__init__()\n",
    "\n",
    "        self.linear_embedding = nn.Linear(input_dim,embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layer)        \n",
    "\n",
    "        self.stabilizer = nn.Sequential(\n",
    "                            nn.Linear(embed_dim, 256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(256, 6)\n",
    "            )\n",
    "        self.joint_rotation_decoder = nn.Sequential(\n",
    "                            nn.Linear(embed_dim, 256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(256, 126)\n",
    "            )\n",
    "\n",
    "        self.body_model = body_model\n",
    "\n",
    "    @staticmethod\n",
    "    def fk_module(global_orientation, joint_rotation, body_model):\n",
    "\n",
    "        global_orientation = utils_transform.sixd2aa(global_orientation.reshape(-1,6)).reshape(global_orientation.shape[0],-1).float()\n",
    "        joint_rotation = utils_transform.sixd2aa(joint_rotation.reshape(-1,6)).reshape(joint_rotation.shape[0],-1).float()\n",
    "        body_pose = body_model(**{'pose_body':joint_rotation, 'root_orient':global_orientation})\n",
    "        joint_position = body_pose.Jtr\n",
    "\n",
    "        return joint_position\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def ik_module(smpl, smpl_jids, target_pose_ids, target_3ds,\n",
    "                         body_pose = None, global_orient = None, transl = None, learning_rate=1e-1, n_iter=5):\n",
    "        target_3ds = target_3ds.view(1, -1, 3)\n",
    "        body_pose_sub = torch.tensor(body_pose[:, target_pose_ids],requires_grad = True)\n",
    "        opti_param = [body_pose_sub]\n",
    "        optimiser = torch.optim.Adam(opti_param, lr = learning_rate)\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            body_pose[:, target_pose_ids] = body_pose_sub\n",
    "            out = smpl(**{'pose_body':body_pose, 'root_orient':global_orient, 'trans': transl})\n",
    "            j_3ds = out.Jtr.view(1, -1, 3)\n",
    "            loss = torch.mean(torch.sqrt(torch.sum(torch.square(j_3ds[:, smpl_jids].squeeze()-target_3ds)[:,[20,21],:],axis=-1)))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimiser.step()\n",
    "            body_pose = body_pose.detach()\n",
    "        return body_pose\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor, do_fk = True):\n",
    "\n",
    "#        embed()\n",
    "        x = self.linear_embedding(input_tensor)\n",
    "        x = x.permute(1,0,2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1,0,2)[:, -1]\n",
    "\n",
    "        global_orientation = self.stabilizer(x)\n",
    "        joint_rotation = self.joint_rotation_decoder(x)\n",
    "        if do_fk:\n",
    "            joint_position = self.fk_module(global_orientation, joint_rotation, self.body_model)\n",
    "            return global_orientation, joint_rotation, joint_position\n",
    "        else:\n",
    "            return global_orientation, joint_rotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rotation_local_full_gt_list', 'hmd_position_global_full_gt_list', 'body_parms_list', 'head_global_trans_list', 'framerate', 'gender', 'filepath', 'position_global_full_gt_world'])\n",
      "torch.Size([2246, 54])\n",
      "torch.Size([2245, 54])\n"
     ]
    }
   ],
   "source": [
    "path_dir = './data_fps60/CMU/test/'\n",
    "file_list = os.listdir(path_dir)\n",
    "file_dir = os.path.join(path_dir,file_list[0])\n",
    "\n",
    "data = np.load(file_dir, allow_pickle=True)\n",
    "\n",
    "f = data['hmd_position_global_full_gt_list']\n",
    "print(data.keys())\n",
    "\n",
    "input_hmd  = f.reshape(f.shape[0], -1)[1:]\n",
    "print(f.shape)\n",
    "print(input_hmd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_fps60/MPI_HDM05/test/1.pkl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_fps60/MPI_HDM05/test/1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Avatar-Compare\\AvatarPoser\\vr_test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Avatar-Compare/AvatarPoser/vr_test.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m file_list[\u001b[39m0\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Avatar-Compare/AvatarPoser/vr_test.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     file_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path_dir, data)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Avatar-Compare/AvatarPoser/vr_test.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(file_dir, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Avatar-Compare/AvatarPoser/vr_test.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     test_loader \u001b[39m=\u001b[39m DataLoader(data[\u001b[39m'\u001b[39m\u001b[39mhmd_position_global_full_gt_list\u001b[39m\u001b[39m'\u001b[39m], batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Avatar-Compare/AvatarPoser/vr_test.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     model \u001b[39m=\u001b[39m AvatarPoser(input_dim\u001b[39m=\u001b[39m\u001b[39m54\u001b[39m, output_dim\u001b[39m=\u001b[39m\u001b[39m132\u001b[39m, num_layer\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, embed_dim\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m, nhead\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, body_model\u001b[39m=\u001b[39mbody_model,device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\rnwns\\anaconda3\\envs\\pytorch_geometric\\lib\\site-packages\\numpy\\lib\\npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    408\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_fps60/MPI_HDM05/test/1'"
     ]
    }
   ],
   "source": [
    "# input_dim, output_dim, num_layer, embed_dim, nhead, body_model, device\n",
    "\n",
    "'''    , \"netG\": {\n",
    "      \"net_type\": \"AvatarPoser\" \n",
    "      , \"num_layer\": 3\n",
    "      , \"input_dim\": 54        // input channel number # 36\n",
    "      , \"output_dim\": 132       // ouput channel number   #132\n",
    "      , \"embed_dim\": 256          // channel number of linear embedding for transformer\n",
    "      , \"nhead\": 8          // \n",
    "      , \"init_type\": \"kaiming_normal\"         // \"orthogonal\" | \"normal\" | \"uniform\" | \"xavier_normal\" | \"xavier_uniform\" | \"kaiming_normal\" | \"kaiming_uniform\"\n",
    "      , \"init_bn_type\": \"uniform\"         // \"uniform\" | \"constant\"\n",
    "      , \"init_gain\": 0.2\n",
    "    }\n",
    "'''\n",
    "\n",
    "pth_path = './model_zoo/avatarposer.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "support_dir = \"./support_data/\"\n",
    "subject_gender = \"male\"\n",
    "bm_fname = os.path.join(support_dir, 'body_models/smplh/{}/model.npz'.format(subject_gender))\n",
    "dmpl_fname = os.path.join(support_dir, 'body_models/dmpls/{}/model.npz'.format(subject_gender))\n",
    "num_betas = 16 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "body_model = BodyModel(bm_fname=bm_fname, num_betas=num_betas, num_dmpls=num_dmpls, dmpl_fname=dmpl_fname).to(device)\n",
    "\n",
    "path_dir = './data_fps60/MPI_HDM05/test/'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "file_dir = os.path.join(path_dir,file_list[0])\n",
    "print(file_dir)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for data in file_list:\n",
    "    file_dir = os.path.join(path_dir, data)\n",
    "    data = np.load(file_dir, allow_pickle=True)\n",
    "\n",
    "    test_loader = DataLoader(data['hmd_position_global_full_gt_list'], batch_size=1,shuffle=False, num_workers=1, drop_last=False, pin_memory=True)\n",
    "    model = AvatarPoser(input_dim=54, output_dim=132, num_layer=3, embed_dim=256, nhead=8, body_model=body_model,device=device)\n",
    "    model.load_state_dict(torch.load(pth_path))\n",
    "    model.to(device)\n",
    "    global_orientation = []\n",
    "    joint_rotation = []\n",
    "    joint_position = []\n",
    "\n",
    "    for index, test_data in enumerate(test_loader) :\n",
    "        test_data = test_data.to('cuda').float()\n",
    "        test_data = test_data.view(1,1,54)\n",
    "\n",
    "        x = model(test_data)\n",
    "\n",
    "        global_orientation.append(x[0].cpu().detach().numpy())\n",
    "        joint_rotation.append(x[1].cpu().detach().numpy())\n",
    "        joint_position.append(x[2].cpu().detach().numpy())\n",
    "        \n",
    "        print(\"idx : {}\".format(index))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = end - start\n",
    "print(\"Runtime is : {:.3f}\".format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_orientation = np.array(global_orientation).squeeze()\n",
    "joint_rotation = np.array(joint_rotation).squeeze()\n",
    "joint_position = np.array(joint_position).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = './test_data.npz'\n",
    "np.savez(npz_path, global_orientation = global_orientation, joint_rotation = joint_rotation, joint_position = joint_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "what = np.load(npz_path)\n",
    "\n",
    "for keys in what:\n",
    "    print(keys)\n",
    "    print(what[keys].shape)\n",
    "\n",
    "print(x[0].shape, x[1].shape, x[2].shape,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11a10ac59cb09bb99373e30fc5a99bd84145ddd99a9af1d11d412ecb91ad6a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
